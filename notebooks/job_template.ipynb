{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table in DB to store the Data. This process will be done by the DE team.\n",
    "\n",
    "import sqlalchemy\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "disk_engine = sqlalchemy.create_engine('sqlite:///data_db.db', echo=False)\n",
    "\n",
    "try:\n",
    "    with disk_engine.connect() as con:\n",
    "        con.execute(\"SELECT 1\")\n",
    "    logger.info('Engine is valid')\n",
    "except Exception as e:\n",
    "    logger.info(f'Engine invalid: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "df_iris = pd.DataFrame(data = np.c_[iris['data'], iris['target']],\n",
    "            columns = iris['feature_names'] + ['target'])\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.columns = df_iris.columns.str.replace(\" \",\"_\").str.replace(\"_\\(cm\\)\",\"\",regex=True)\n",
    "df_iris['target'] = df_iris['target'].astype(int)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.to_sql(name='iris', \n",
    "            con = disk_engine,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'sepal_length': sqlalchemy.types.Float(precision=4, asdecimal=True),\n",
    "                   'sepal_width': sqlalchemy.types.Float(precision=4, asdecimal=True),\n",
    "                   'petal_length': sqlalchemy.types.Float(precision=4, asdecimal=True),\n",
    "                   'petal_width': sqlalchemy.types.Float(precision=4, asdecimal=True),\n",
    "                   'target': sqlalchemy.types.INTEGER()},\n",
    "            chunksize=100,\n",
    "            method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the modelling process which we will run in a pipeline whenever there is a data refresh happing or due to manual trigger.\n",
    "\n",
    "import sqlalchemy\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression_to_json(lrmodel, file=None):\n",
    "    if file is not None:\n",
    "        serialize = lambda x: json.dump(x, file)\n",
    "    else:\n",
    "        serialize = json.dumps\n",
    "    data = {}\n",
    "    data['init_params'] = lrmodel.get_params()\n",
    "    data['model_params'] = mp = {}\n",
    "    for p in ('coef_', 'intercept_','classes_', 'n_iter_'):\n",
    "        mp[p] = getattr(lrmodel, p).tolist()\n",
    "    return serialize(data)\n",
    "\n",
    "\n",
    "disk_engine = sqlalchemy.create_engine('sqlite:///data_db.db', echo=False)\n",
    "\n",
    "try:\n",
    "    with disk_engine.connect() as con:\n",
    "        con.execute(\"SELECT 1\")\n",
    "    logger.info('engine is valid')\n",
    "except Exception as e:\n",
    "    logger.info(f'Engine invalid: {str(e)}')\n",
    "\n",
    "\n",
    "iris_df_from_db = pd.read_sql_query('SELECT * FROM iris', disk_engine)\n",
    "iris_df_from_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris_df_from_db.drop(columns = ['target'])\n",
    "y = iris_df_from_db[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X, y)\n",
    "model_response = logistic_regression_to_json(lr_model)\n",
    "model_id = str(uuid.uuid4())\n",
    "logger.info(f\"Storing results for : {model_id}.\")\n",
    "parameters_df = pd.DataFrame({'model_id':[model_id], \"response\":[model_response] })\n",
    "parameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_df['model_id'] = parameters_df['model_id'].astype(str)\n",
    "parameters_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_df.to_sql(name='model_parameters',\n",
    "                    con= disk_engine,\n",
    "                    if_exists='replace',\n",
    "                    index=False,\n",
    "                    dtype={'response': sqlalchemy.types.JSON(), \n",
    "                           'model_id': sqlalchemy.types.String()},\n",
    "                    chunksize=100,\n",
    "                    method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression_from_json(jstring):\n",
    "    data = json.loads(jstring)\n",
    "    model = LogisticRegression(**data['init_params'])\n",
    "    for name, p in data['model_params'].items():\n",
    "        setattr(model, name, np.array(p))\n",
    "    return model\n",
    "\n",
    "disk_engine = sqlalchemy.create_engine('sqlite:///data_db.db', echo=False)\n",
    "\n",
    "try:\n",
    "    with disk_engine.connect() as con:\n",
    "        con.execute(\"SELECT 1\")\n",
    "    logger.info('Engine is valid.')\n",
    "except Exception as e:\n",
    "    logger.info(f'Engine invalid: {str(e)}')\n",
    "\n",
    "iris_df_from_db = pd.read_sql_query('SELECT * FROM iris', disk_engine)\n",
    "X = iris_df_from_db.drop(columns = ['target'])\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters_from_db = pd.read_sql_query('SELECT * FROM model_parameters', disk_engine)\n",
    "d = eval((model_parameters_from_db[model_parameters_from_db['model_id'] == model_id]['response'][0]))\n",
    "model_parameters_json = d.replace(\"'\", \"\\\"\")\n",
    "model_parameters_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_object = logistic_regression_from_json(model_parameters_json)\n",
    "model_object.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "333f3af9d2be23e5ba6b94745902dd399d9c71edafea2d6f880817098483d6f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mroi_ml_lib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
